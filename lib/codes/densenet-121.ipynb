{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 30 classes.\n",
      "Found 2000 images belonging to 30 classes.\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NAMITHAA\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.2653 - loss: 2.8073\n",
      "Epoch 1: val_accuracy improved from -inf to 0.77520, saving model to C:/Users/NAMITHAA/Downloads/save_models.keras\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m717s\u001b[0m 3s/step - accuracy: 0.2661 - loss: 2.8043 - val_accuracy: 0.7752 - val_loss: 0.9972 - learning_rate: 1.0000e-04\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python312\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: val_accuracy did not improve from 0.77520\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.7500 - val_loss: 1.1488 - learning_rate: 1.0000e-04\n",
      "Epoch 3/10\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7324 - loss: 0.9678\n",
      "Epoch 3: val_accuracy improved from 0.77520 to 0.84476, saving model to C:/Users/NAMITHAA/Downloads/save_models.keras\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m702s\u001b[0m 3s/step - accuracy: 0.7325 - loss: 0.9673 - val_accuracy: 0.8448 - val_loss: 0.5658 - learning_rate: 1.0000e-04\n",
      "Epoch 4/10\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.84476\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.6875 - val_loss: 1.1054 - learning_rate: 1.0000e-04\n",
      "Epoch 5/10\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - accuracy: 0.8231 - loss: 0.6285\n",
      "Epoch 5: val_accuracy improved from 0.84476 to 0.88357, saving model to C:/Users/NAMITHAA/Downloads/save_models.keras\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1150s\u001b[0m 4s/step - accuracy: 0.8232 - loss: 0.6284 - val_accuracy: 0.8836 - val_loss: 0.4207 - learning_rate: 1.0000e-04\n",
      "Epoch 6/10\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.88357\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.6875 - val_loss: 0.8546 - learning_rate: 1.0000e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.8610 - loss: 0.4797\n",
      "Epoch 7: val_accuracy improved from 0.88357 to 0.88659, saving model to C:/Users/NAMITHAA/Downloads/save_models.keras\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m991s\u001b[0m 4s/step - accuracy: 0.8610 - loss: 0.4796 - val_accuracy: 0.8866 - val_loss: 0.3783 - learning_rate: 1.0000e-04\n",
      "Epoch 8/10\n",
      "\n",
      "Epoch 8: val_accuracy improved from 0.88659 to 0.93750, saving model to C:/Users/NAMITHAA/Downloads/save_models.keras\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 37ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.9375 - val_loss: 0.3468 - learning_rate: 1.0000e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.8900 - loss: 0.3857\n",
      "Epoch 9: val_accuracy did not improve from 0.93750\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m947s\u001b[0m 4s/step - accuracy: 0.8900 - loss: 0.3857 - val_accuracy: 0.8931 - val_loss: 0.3503 - learning_rate: 1.0000e-04\n",
      "Epoch 10/10\n",
      "\n",
      "Epoch 10: val_accuracy improved from 0.93750 to 1.00000, saving model to C:/Users/NAMITHAA/Downloads/save_models.keras\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 35ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.1046 - learning_rate: 1.0000e-04\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 1s/step - accuracy: 0.8929 - loss: 0.3617\n",
      "Test accuracy: 90.76%\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "import os\n",
    "\n",
    "#######Data set preprocessing ########\n",
    " # Set dataset directory and parameters\n",
    "dataset_dir = \"C:/Users/NAMITHAA/Downloads/eee_dataset/AID\"\n",
    "target_size = (224, 224)  # Target size for DenseNet121\n",
    "batch_size = 32\n",
    "\n",
    "# Define the data generators with increased augmentation for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255.0,\n",
    "    rotation_range=45,  # Increased rotation range\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.2  # Use 20% of the data for validation\n",
    ")\n",
    "\n",
    "# Data generators for training and validation\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    dataset_dir,\n",
    "    target_size=target_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training'  # Set as training data\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    dataset_dir,\n",
    "    target_size=target_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'  # Set as validation data\n",
    ")\n",
    "\n",
    "####### Model Building: #######\n",
    "\n",
    "   # Load DenseNet121 with pre-trained ImageNet weights\n",
    "base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Add custom layers on top of DenseNet121\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(256, activation='relu')(x)  # Reduced the number of units\n",
    "x = Dropout(0.3)(x)  # Reduced dropout rate\n",
    "predictions = Dense(train_generator.num_classes, activation='softmax')(x)\n",
    "\n",
    "# Create the final model\n",
    "model = tf.keras.models.Model(inputs=base_model.input, outputs=predictions)#defining the input and output layers.​\n",
    "\n",
    "######## Model Training #########\n",
    "\n",
    "# Fine-tune more layers of DenseNet121\n",
    "for layer in base_model.layers[:-50]:\n",
    "    layer.trainable = False #freezing top layers \n",
    "for layer in base_model.layers[-50:]:\n",
    "    layer.trainable = True #proceed with  second part \n",
    "\n",
    "# Compile the model with a lower learning rate and learning rate scheduler\n",
    "optimizer = Adam(learning_rate=0.0001)  # Lower learning rate\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define callbacks\n",
    "filepath = \"C:/Users/NAMITHAA/Downloads/save_models.keras\"\n",
    "\n",
    "#When the monitored metric improves, it saves the model weights\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "# adjusts the learning rate during training based on changes in the validation loss, aiming to improve model performance\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, min_lr=0.000001, verbose=1)\n",
    "\n",
    "#stops training if the validation loss does not improve for 15 consecutive epochs\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "\n",
    "# Train the model with more epochs\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    epochs=10,  \n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // batch_size,\n",
    "    callbacks=[checkpoint, early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "#####Evalvation####\n",
    " # Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(validation_generator, steps=validation_generator.samples // batch_size)\n",
    "print(f'Test accuracy: {test_accuracy * 100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
